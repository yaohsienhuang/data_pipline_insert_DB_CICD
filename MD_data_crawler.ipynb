{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ftplib import FTP\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from io import BytesIO,StringIO\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FTP_download(date, date_folder):\n",
    "    ftp = FTP()\n",
    "    timeout = 30\n",
    "    port = 21\n",
    "    ftp.connect('xx.xx.xx.xx', port, timeout) # 連線FTP伺服器\n",
    "    ftp.login('xx', 'xx**') # 登入\n",
    "    temp_list = ftp.nlst(date_folder)       # 獲得目錄列表\n",
    "    temp_list = sorted(temp_list, reverse = True)\n",
    "    logfile.append(f\"{datetime.now()} [success] -> 獲取資料夾 : {temp_list}\")\n",
    "    print(f\"{datetime.now()} [success] -> 獲取資料夾 : {temp_list}\")\n",
    "    date_list = []\n",
    "\n",
    "    #收集 +/-1hr的資料\n",
    "    date_shift1hr=date[:-2]+str(int(date[-2:])-1).zfill(2)\n",
    "    for folder in temp_list:\n",
    "        if (date in os.path.basename(folder)) | (date_shift1hr in os.path.basename(folder)): \n",
    "            date_list.append(folder)\n",
    "    \n",
    "    logfile.append(f\"{datetime.now()} [success] -> 篩選資料區間 : {date_shift1hr} <-> {date}\")\n",
    "    print(f\"{datetime.now()} [success] -> 篩選資料區間 : {date_shift1hr} <-> {date}\")\n",
    "\n",
    "    if date_list:\n",
    "        remote_folder = date_list[0] + '/MD/'# 最新日期\n",
    "        remote_folder_name= date_list[0].split('/')[-1]\n",
    "        logfile.append(f\"{datetime.now()} [success] -> 處理資料夾內容 : {remote_folder}\")\n",
    "        print(f\"{datetime.now()} [success] -> 處理資料夾內容 : {remote_folder}\")\n",
    "        remote_list = ftp.nlst(remote_folder) # 獲得目錄列表\n",
    "\n",
    "        #先建立必要資料的完整路徑清單\n",
    "        necessary_list=[]\n",
    "        for doc in dataDict.keys():\n",
    "            necessary_list.append(remote_folder+doc+\".csv\")\n",
    "\n",
    "        for path in necessary_list:\n",
    "            file_name = os.path.basename(path).split('.')[0]\n",
    "            #確認是否有缺少檔案\n",
    "            if path not in remote_list:\n",
    "                if \"WipOutPlanTime\" in path: \n",
    "                    for onepath in remote_list:\n",
    "                        if \"WipOutPlanTime\" in onepath:\n",
    "                            path=onepath\n",
    "                else:\n",
    "                    #print(path+\" : 找不到資料\")\n",
    "                    logfile.append(f\"{datetime.now()} [err] -> {path} : 找不到資料\")\n",
    "                    print(f\"{datetime.now()} [err] -> {path} : 找不到資料\")\n",
    "                    save_log()\n",
    "                    continue\n",
    "            \n",
    "            #print(path+\" -> 開始進行讀檔\")\n",
    "            logfile.append(f\"{datetime.now()} [success] -> {path} : 開始進行讀檔\")\n",
    "            print(f\"{datetime.now()} [success] -> {path} : 開始進行讀檔\")\n",
    "\n",
    "            try : \n",
    "                flo = BytesIO()\n",
    "                ftp.retrbinary('RETR ' + path, flo.write)\n",
    "                flo.seek(0)\n",
    "\n",
    "                if file_name==\"uph\": #uph中header特別亂需要整理後再合併\n",
    "                    flo_header = BytesIO()\n",
    "                    ftp.retrbinary('RETR ' + path, flo_header.write)\n",
    "                    flo_header.seek(0)\n",
    "                    \n",
    "                    header_0=pd.read_csv(flo_header,sep='\\t', index_col=0, nrows=0).columns.tolist()[0]\n",
    "                    header_1=header_0.replace('(ea/hr,set)','').replace('(K/day,set)','').replace('(K/day)','').replace(' ','_').replace('.','_')\n",
    "                    header_2=pd.read_csv(StringIO(header_1),sep=',', engine='python',encoding = \"ISO-8859-1\")#encoding = \"ISO-8859-1\" utf-8\n",
    "                    header_2=header_2.loc[:,~header_2.columns.str.contains('^Unnamed')]\n",
    "                    header_3=header_2.columns.tolist()\n",
    "\n",
    "                    df=pd.read_csv(flo, sep=',', engine='python',encoding = \"ISO-8859-1\",skipinitialspace=True)\n",
    "                    df = df.iloc[:,0:len(header_3)]\n",
    "                    df.columns=header_3\n",
    "                                        \n",
    "                else:\n",
    "                    df=pd.read_csv(flo, sep=',', engine='python',encoding = \"ISO-8859-1\",skipinitialspace=True)\n",
    "\n",
    "                df=df.replace('\\t','', regex=True)\n",
    "                df=df.replace(' ','', regex=True)\n",
    "                df=df.replace('','NaN', regex=True)\n",
    "                df=df.fillna('NaN')\n",
    "                df=df.replace('nan','NaN', regex=True)\n",
    "                df=df.replace('None','NaN', regex=True)\n",
    "                df=df.loc[:,~df.columns.str.contains('^Unnamed')]\n",
    "                dataDict[file_name][\"header\"].append(df.columns.tolist())\n",
    "                dataDict[file_name][\"time\"]=remote_folder_name\n",
    "                for index, row in df.iterrows():\n",
    "                    dataDict[file_name][\"data\"].append(row.tolist())\n",
    "\n",
    "                logfile.append(f\"{datetime.now()} [success] -> {path} : 讀檔成功\")\n",
    "                print(f\"{datetime.now()} [success] -> {path} : 讀檔成功\")\n",
    "            except Exception as e:\n",
    "                logfile.append(f\"{datetime.now()} [err] -> {path} : 讀檔失敗，異常訊息[{e}]\")\n",
    "                print(f\"{datetime.now()} [err] -> {path} : 讀檔失敗，異常訊息[{e}]\")\n",
    "                save_log()\n",
    "                raise Exception(e)\n",
    "        \n",
    "    else :\n",
    "        logfile.append(f\"{datetime.now()} [err] -> {date_folder} : 內無資料夾\")\n",
    "        print(f\"{datetime.now()} [err] -> {date_folder} : 內無資料夾\")\n",
    "        save_log()\n",
    "\n",
    "    ftp.quit()\n",
    "    return True\n",
    "\n",
    "def executeSQL(sql_list):\n",
    "    try:\n",
    "        db = psycopg2.connect(host=\"xx.xx.xx.xx\",user=\"postgres\",password=\"admin\",dbname=\"APS_MD\",sslmode='allow')            \n",
    "        cursor = db.cursor()\n",
    "        for i in range(len(sql_list)):\n",
    "            cursor.execute(sql_list[i])\n",
    "    except Exception as e:\n",
    "        logfile.append(f\"{datetime.now()} [err] -> {sql_list[i]} : executeSQL失敗\")s\n",
    "        print(f\"{datetime.now()} [err] -> {sql_list[i]} : executeSQL失敗\")\n",
    "        save_log()\n",
    "        raise Exception(e)\n",
    "    finally:\n",
    "        db.commit()\n",
    "        db.close()\n",
    "\n",
    "def readSQL(sql_list):\n",
    "    db = psycopg2.connect(host=\"10.3.10.203\",user=\"postgres\",password=\"admin20^\",dbname=\"APS_MD\",sslmode='allow')            \n",
    "    data=pd.read_sql(sql_list,db)\n",
    "    db.commit()\n",
    "    db.close()\n",
    "    return data\n",
    "\n",
    "def save_log():\n",
    "    df_logfile = pd.DataFrame(logfile, columns=[\"log\"])\n",
    "    df_logfile.to_csv(f'log/log-{year+date+time_stamp}.csv', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "def insert_run_list():\n",
    "    result_detail=[]\n",
    "    for key, value in dataDict.items():\n",
    "        result_detail.append(key+\":[\"+value[\"success\"]+\"]\")\n",
    "    result_string = \";\".join(result_detail)\n",
    "    result=\"success\" if \"X\" not in result_string else \"fail\"\n",
    "    result_timestamp=dataDict[\"eim\"][\"time\"]\n",
    "    executeSQL([str(f\"INSERT INTO public.data_crawler_run_list(update_time,scope,result,result_detail,timestamp) VALUES('{today}','mes','{result}','{result_string}','{result_timestamp}')\")])\n",
    "\n",
    "def insertDatatoDB():\n",
    "    for key, value in dataDict.items():\n",
    "        if len(dataDict[key][\"data\"])>0:\n",
    "            try:\n",
    "                insert_table=value[\"table\"]\n",
    "                insert_col=value[\"header\"][0]\n",
    "                insert_data=value[\"data\"]\n",
    "                insert_time=value[\"time\"]\n",
    "                logfile.append(f\"{datetime.now()} [success] -> {insert_table} : 處理中...  \")\n",
    "                print(f\"{datetime.now()} [success] -> {insert_table} : 處理中...  \")\n",
    "\n",
    "                # selectSQL=str(f\"select * from public.{insert_table}\")\n",
    "                # print(readSQL(selectSQL))\n",
    "\n",
    "                # (1)先刪除資料庫中timestamp相同者\n",
    "                deleteSQL=str(f\"delete from public.{insert_table} where timestamp='{insert_time}'\")\n",
    "                executeSQL([deleteSQL])\n",
    "                logfile.append(f\"{datetime.now()} [success] -> {insert_table} : 完成delete timestamp重複者\")\n",
    "                print(f\"{datetime.now()} [success] -> {insert_table} : 完成delete timestamp重複者\")\n",
    "\n",
    "                # (2)資料庫中的欄位active=\"N\"\n",
    "                activeChangeSQL=str(f\"update public.{insert_table} set active='N'\")\n",
    "                executeSQL([activeChangeSQL])\n",
    "                logfile.append(f\"{datetime.now()} [success] -> {insert_table} : 完成將原始資料回壓active='N'\")\n",
    "                print(f\"{datetime.now()} [success] -> {insert_table} : 完成將原始資料回壓active='N'\")\n",
    "\n",
    "                # (3)insert 新的資料進去\n",
    "                col_num=len(insert_col)\n",
    "                col_content=\"\"\n",
    "                for i in range(col_num):\n",
    "                    col_content+='%s,'\n",
    "                \n",
    "                col_content=col_content[:-1] #去除最後一個\",\"\"\n",
    "                insert_sql_list=[]\n",
    "                for j in range(len(insert_data)):\n",
    "                    insert_sql_list.append(str(f\"INSERT INTO public.{insert_table}({insert_col},timestamp,active)\").replace(\"'\",\"\").replace('[','').replace(']','')+str(f\" VALUES({insert_data[j]},'{insert_time}','Y')\").replace('[','').replace(']',''))\n",
    "                executeSQL(insert_sql_list)\n",
    "                logfile.append(f\"{datetime.now()} [success] -> {insert_table} : 完成新資料insert({insert_time})\")\n",
    "                print(f\"{datetime.now()} [success] -> {insert_table} : 完成新資料insert({insert_time})\")\n",
    "                dataDict[key][\"success\"]=\"O\"\n",
    "\n",
    "            except Exception as e:\n",
    "                logfile.append(f\"{datetime.now()} [err] -> {insert_table} : 異常訊息[{e}]\")\n",
    "                print(f\"{datetime.now()} [err] -> {insert_table} : 異常訊息[{e}]\")\n",
    "                save_log()\n",
    "                raise Exception(e)\n",
    "        else :\n",
    "            logfile.append(f\"{datetime.now()} -> dataDict沒有資料[{key}] : 跳過 DB execute\")\n",
    "            print(f\"{datetime.now()} -> dataDict沒有資料[{key}] : 跳過 DB execute\")\n",
    "            save_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get now date & time \n",
    "today=datetime.now()\n",
    "year=str(today.year)\n",
    "month=str(today.month).zfill(2)\n",
    "day=str(today.day).zfill(2)\n",
    "hour=str(today.hour).zfill(2)\n",
    "hour_before_1hr=str(today.hour-1).zfill(2)\n",
    "time_stamp=hour+str(today.minute).zfill(2)+str(today.second).zfill(2)\n",
    "date=month+day\n",
    "dateTime=year+date+hour\n",
    "\n",
    "# create a array to append data & header, and start to data pipline \n",
    "dataDict={\n",
    "    \"eim\":{\"header\":[],\"data\":[],\"table\":\"mes_eim\",\"time\":\"\",\"success\":\"X\"},\n",
    "    \"gtsdat_STACKDIEOPER\":{\"header\":[],\"data\":[],\"table\":\"mes_gtsdat_stackdieoper\",\"time\":\"\",\"success\":\"X\"},\n",
    "    \"ntcent\":{\"header\":[],\"data\":[],\"table\":\"mes_ntcent\",\"time\":\"\",\"success\":\"X\"},\n",
    "    \"ntclot\":{\"header\":[],\"data\":[],\"table\":\"mes_ntclot\",\"time\":\"\",\"success\":\"X\"},\n",
    "    \"uph\":{\"header\":[],\"data\":[],\"table\":\"mes_uph\",\"time\":\"\",\"success\":\"X\"},\n",
    "    \"WipOutPlanTime\":{\"header\":[],\"data\":[],\"table\":\"mes_wipoutplantime\",\"time\":\"\",\"success\":\"X\"},\n",
    "}\n",
    "\n",
    "# log\n",
    "logfile=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MES_FTP/AutoScheduling/2022/0602/20220602080000/MD/eim.csv -> 開始進行讀檔\n",
      "MES_FTP/AutoScheduling/2022/0602/20220602080000/MD/gtsdat_STACKDIEOPER.csv -> 開始進行讀檔\n",
      "MES_FTP/AutoScheduling/2022/0602/20220602080000/MD/ntcent.csv -> 開始進行讀檔\n",
      "MES_FTP/AutoScheduling/2022/0602/20220602080000/MD/ntclot.csv -> 開始進行讀檔\n",
      "MES_FTP/AutoScheduling/2022/0602/20220602080000/MD/uph.csv -> 開始進行讀檔\n",
      "MES_FTP/AutoScheduling/2022/0602/20220602080000/MD/WipOutPlanTime_20220602080636.csv -> 開始進行讀檔\n",
      "mes_eim -> 開始進行DB execute\n",
      "mes_gtsdat_stackdieoper -> 開始進行DB execute\n",
      "mes_ntcent -> 開始進行DB execute\n",
      "mes_ntclot -> 開始進行DB execute\n",
      "mes_uph -> 開始進行DB execute\n",
      "mes_wipoutplantime -> 開始進行DB execute\n"
     ]
    }
   ],
   "source": [
    "# download & process data\n",
    "logfile.append(f\"{datetime.now()} -> 開始進行資料讀取與處理程序...\")\n",
    "print(f\"{datetime.now()} -> 開始進行資料讀取與處理程序...\")\n",
    "FTP_download(dateTime,f'MES_FTP/AutoScheduling/{year}/{date}/')\n",
    "logfile.append(f\"{datetime.now()} -> 完成資料讀取與處理程序。\")\n",
    "print(f\"{datetime.now()} -> 完成資料讀取與處理程序。\")\n",
    "\n",
    "# insert processed data to DB tables\n",
    "logfile.append(f\"{datetime.now()} -> 開始進行DB execute...\")\n",
    "print(f\"{datetime.now()} -> 開始進行DB execute...\")\n",
    "insertDatatoDB()\n",
    "logfile.append(f\"{datetime.now()} -> 完成進行DB execute。\")\n",
    "print(f\"{datetime.now()} -> 完成進行DB execute。\")\n",
    "\n",
    "#finally (1)save log (2)update data_crawler_run_list\n",
    "save_log()\n",
    "insert_run_list()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
